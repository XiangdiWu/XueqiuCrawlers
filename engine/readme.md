# Engine æ ¸å¿ƒæ¨¡å—

é›ªçƒçˆ¬è™«é¡¹ç›®çš„æ ¸å¿ƒå¼•æ“æ¨¡å—ï¼Œæä¾›è®¤è¯ã€æ•°æ®å­˜å‚¨ã€çˆ¬è™«æœåŠ¡ç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚

## ğŸ“ æ¨¡å—ç»“æ„

```
engine/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ README.md                 # æœ¬æ–‡æ¡£
â”œâ”€â”€ auto_cookie.py           # è‡ªåŠ¨Cookieç”Ÿæˆå™¨
â”œâ”€â”€ crawler_service.py       # çˆ¬è™«æœåŠ¡å±‚
â”œâ”€â”€ csv_storage.py          # CSVå­˜å‚¨ç®¡ç†å™¨
â”œâ”€â”€ database.py             # æ•°æ®åº“è¿æ¥å’Œæ“ä½œ
â”œâ”€â”€ logger.py               # æ—¥å¿—é…ç½®æ¨¡å—
â”œâ”€â”€ xueqiu_auth.py         # é›ªçƒè®¤è¯ç³»ç»Ÿ
â””â”€â”€ xueqiu_deobfuscator.js  # JavaScriptåæ··æ·†ä»£ç 
```

## ğŸ”§ æ ¸å¿ƒç»„ä»¶

### 1. è®¤è¯ç³»ç»Ÿ (`xueqiu_auth.py`)

é›ªçƒè®¤è¯ç®¡ç†å™¨ï¼Œè´Ÿè´£Cookieçš„è·å–ã€éªŒè¯å’Œä¼šè¯ç®¡ç†ã€‚

#### ä¸»è¦åŠŸèƒ½ï¼š
- âœ… è‡ªåŠ¨Cookieç”Ÿæˆå’ŒéªŒè¯
- âœ… è®¤è¯çŠ¶æ€æ£€æŸ¥ï¼ˆæ¸¸å®¢/ç™»å½•çŠ¶æ€ï¼‰
- âœ… ä¼šè¯ç®¡ç†å’Œèµ„æºä¼˜åŒ–
- âœ… Cookieè¿‡æœŸå¤„ç†

#### ä½¿ç”¨ç¤ºä¾‹ï¼š
```python
from engine.xueqiu_auth import get_auth, get_authenticated_session

# è·å–è®¤è¯çŠ¶æ€
auth = get_auth()
status = auth.get_auth_status()
print(f"è®¤è¯çŠ¶æ€: {status['message']}")

# è·å–å·²è®¤è¯çš„ä¼šè¯
session = get_authenticated_session()
response = session.get('https://xueqiu.com')
```

### 2. è‡ªåŠ¨Cookieç”Ÿæˆå™¨ (`auto_cookie.py`)

åŸºäºé€†å‘å·¥ç¨‹çš„Cookieè‡ªåŠ¨ç”Ÿæˆï¼Œä¸“æ³¨äºåçˆ¬è™«å‚æ•°å¤„ç†ã€‚

#### ä¸»è¦åŠŸèƒ½ï¼š
- ğŸª è‡ªåŠ¨è·å–é›ªçƒåŸºç¡€Cookie
- ğŸ”’ ç”Ÿæˆacw_sc__v2åçˆ¬è™«å‚æ•°
- âš¡ JavaScriptæ‰§è¡Œä¼˜åŒ–ï¼ˆè¶…æ—¶æ§åˆ¶ã€èµ„æºç®¡ç†ï¼‰
- ğŸ›¡ï¸ å¤‡ç”¨ç”Ÿæˆç®—æ³•

#### ä½¿ç”¨ç¤ºä¾‹ï¼š
```python
from engine.auto_cookie import get_auto_cookie_generator

generator = get_auto_cookie_generator()
cookies = generator.generate_fresh_cookies()
```

### 3. æ•°æ®å­˜å‚¨ç³»ç»Ÿ

#### 3.1 æ•°æ®åº“ç®¡ç† (`database.py`)

é«˜æ€§èƒ½æ•°æ®åº“è¿æ¥æ± å’Œæ“ä½œæ¥å£ã€‚

**ç‰¹æ€§ï¼š**
- ğŸŠ è¿æ¥æ± å¤ç”¨ï¼ˆé»˜è®¤5ä¸ªè¿æ¥ï¼Œæœ€å¤§10ä¸ªï¼‰
- ğŸ”— çº¿ç¨‹å®‰å…¨çš„è¿æ¥ç®¡ç†
- âš¡ æ‰¹é‡æ“ä½œæ”¯æŒ
- ğŸ›¡ï¸ è¿æ¥æœ‰æ•ˆæ€§æ£€æŸ¥

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```python
from engine.database import DatabaseManager, StockRepository

# åˆ›å»ºæ•°æ®åº“ç®¡ç†å™¨
db_manager = DatabaseManager(pool_size=5)

# ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨
with db_manager.get_connection() as conn:
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM stocks")
    result = cursor.fetchall()
```

#### 3.2 CSVå­˜å‚¨ç®¡ç†å™¨ (`csv_storage.py`)

é«˜æ•ˆçš„CSVæ–‡ä»¶å­˜å‚¨ï¼Œæ”¯æŒå¤§æ•°æ®å¤„ç†ã€‚

**ç‰¹æ€§ï¼š**
- ğŸ“¦ åˆ†å—è¯»å†™ï¼ˆ10Kå†™å…¥/50Kè¯»å–ï¼‰
- ğŸ—‚ï¸ è‡ªåŠ¨ç›®å½•ç®¡ç†
- ğŸ“Š å¤§æ–‡ä»¶æ£€æµ‹ï¼ˆ50MBé˜ˆå€¼ï¼‰
- ğŸ’¾ å¤‡ä»½å’Œå¯¼å‡ºåŠŸèƒ½

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```python
from engine.csv_storage import CSVStorage

# åˆ›å»ºå­˜å‚¨ç®¡ç†å™¨
storage = CSVStorage(csv_path='data/csv')

# ä¿å­˜æ•°æ®ï¼ˆæ”¯æŒåˆ†å—ï¼‰
storage.save_to_csv(data, 'stock_list', chunk_size=10000)

# æŒ‰æ—¥æœŸä¿å­˜Kçº¿æ•°æ®
storage.save_kline_data_by_date(kline_data, '2023-12-01')
```

### 4. ç»Ÿä¸€æ•°æ®ä»“åº“ (`database.py` ä¸­çš„ `DataRepository`)

æä¾›ç»Ÿä¸€çš„å­˜å‚¨æ¥å£ï¼Œæ”¯æŒæ•°æ®åº“å’ŒCSVä¸¤ç§æ¨¡å¼ã€‚

**ç‰¹æ€§ï¼š**
- ğŸ”„ å­˜å‚¨ç±»å‹åˆ‡æ¢ï¼ˆdatabase/csvï¼‰
- ğŸ“Š å­˜å‚¨ä¿¡æ¯æŸ¥è¯¢
- ğŸ“ æ‰¹é‡æ•°æ®æ“ä½œ
- ğŸ’¾ è‡ªåŠ¨å¤‡ä»½

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```python
from engine.database import DataRepository

# åˆ›å»ºæ•°æ®ä»“åº“ï¼ˆè‡ªåŠ¨ä»é…ç½®è¯»å–å­˜å‚¨ç±»å‹ï¼‰
repo = DataRepository()

# ä¿å­˜è‚¡ç¥¨æ•°æ®
repo.save_stock_basic_info(stock_data)

# è·å–æœªå¤„ç†çš„è‚¡ç¥¨åˆ—è¡¨
unprocessed = repo.get_unprocessed_financial_stocks()
```

### 5. çˆ¬è™«æœåŠ¡å±‚ (`crawler_service.py`)

é«˜çº§çˆ¬è™«æœåŠ¡ï¼Œå°è£…æ‰€æœ‰çˆ¬è™«æ“ä½œã€‚

**åŠŸèƒ½æ¨¡å—ï¼š**
- ğŸ“ˆ è‚¡ç¥¨åˆ—è¡¨çˆ¬å–
- ğŸ¢ å…¬å¸ä¿¡æ¯çˆ¬å–  
- ğŸ’° è´¢åŠ¡æ•°æ®çˆ¬å–
- ğŸ“Š Kçº¿æ•°æ®çˆ¬å–
- ğŸ”„ å­˜å‚¨ç±»å‹åˆ‡æ¢

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```python
from engine.crawler_service import CrawlerService

# åˆ›å»ºçˆ¬è™«æœåŠ¡
service = CrawlerService(storage_type='database')

# æ‰§è¡Œå®Œæ•´çˆ¬å–æµç¨‹
service.run_full_crawl()

# æˆ–å•ç‹¬æ‰§è¡Œç‰¹å®šçˆ¬å–
service.run_stock_list_crawl()
service.run_company_info_crawl()
```

### 6. æ—¥å¿—ç³»ç»Ÿ (`logger.py`)

ç»Ÿä¸€çš„æ—¥å¿—é…ç½®å’Œç®¡ç†ã€‚

**ç‰¹æ€§ï¼š**
- ğŸ“ ç»“æ„åŒ–æ—¥å¿—æ ¼å¼
- ğŸ“ è‡ªåŠ¨æ—¥å¿—æ–‡ä»¶ç®¡ç†
- ğŸ›ï¸ å¯é…ç½®æ—¥å¿—çº§åˆ«
- ğŸ–¥ï¸ æ§åˆ¶å°å’Œæ–‡ä»¶åŒé‡è¾“å‡º

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```python
from engine.logger import get_logger

logger = get_logger(__name__)
logger.info("æ“ä½œå¼€å§‹")
logger.error("å‘ç”Ÿé”™è¯¯", exc_info=True)
```

## âš™ï¸ é…ç½®è¯´æ˜

Engineæ¨¡å—ä¾èµ– `config/settings.py` ä¸­çš„é…ç½®ï¼š

```python
# æ•°æ®åº“é…ç½®
DATABASE_CONFIG = {
    'host': 'localhost',
    'user': 'root',
    'password': '',
    'database': 'xueqiu',
    'charset': 'utf8mb4'
}

# å­˜å‚¨é…ç½®
STORAGE_CONFIG = {
    'type': 'database',  # 'database' æˆ– 'csv'
    'csv_path': 'data/csv',
    'csv_encoding': 'utf-8-sig',
    'backup_path': 'data/backup'
}

# çˆ¬è™«é…ç½®
CRAWLER_CONFIG = {
    'max_retries': 3,
    'timeout': 30,
    'request_delay': 1,
    'page_size': 100
}

# æ—¥å¿—é…ç½®
LOG_CONFIG = {
    'level': 'INFO',
    'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    'filename': 'logs/xueqiu_crawler.log'
}
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### å·²å®æ–½çš„ä¼˜åŒ–æªæ–½ï¼š

1. **è¿æ¥æ± ä¼˜åŒ–** - å‡å°‘50-80%æ•°æ®åº“è¿æ¥å¼€é”€
2. **åˆ†å—å¤„ç†** - æ”¯æŒGBçº§æ•°æ®æ–‡ä»¶å¤„ç†
3. **ä¼šè¯ç®¡ç†** - 1å°æ—¶ä¼šè¯è¿‡æœŸï¼Œé¿å…èµ„æºæ³„æ¼
4. **JavaScriptæ‰§è¡Œ** - 5ç§’è¶…æ—¶ï¼Œè¿›ç¨‹ç»„ç®¡ç†
5. **å†…å­˜ä¼˜åŒ–** - å¤§æ–‡ä»¶æµå¼å¤„ç†ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º

### æ€§èƒ½æŒ‡æ ‡ï¼š

- ğŸŠ **æ•°æ®åº“è¿æ¥**: æ± åŒ–å¤ç”¨ï¼Œæœ€å¤§å¹¶å‘10è¿æ¥
- ğŸ“¦ **CSVå¤„ç†**: 10Kæ¡/å—å†™å…¥ï¼Œ50Kæ¡/å—è¯»å–
- â±ï¸ **è¯·æ±‚è¶…æ—¶**: JavaScript 5ç§’ï¼ŒHTTP 30ç§’
- ğŸ’¾ **å†…å­˜æ§åˆ¶**: 50MBæ–‡ä»¶è‡ªåŠ¨åˆ†å—å¤„ç†

## ğŸ”„ ä½¿ç”¨æµç¨‹

### å…¸å‹ä½¿ç”¨åœºæ™¯ï¼š

1. **åˆå§‹åŒ–è®¤è¯**
   ```python
   from engine.xueqiu_auth import get_auth
   auth = get_auth()
   status = auth.get_auth_status()
   ```

2. **åˆ›å»ºçˆ¬è™«æœåŠ¡**
   ```python
   from engine.crawler_service import CrawlerService
   service = CrawlerService()
   ```

3. **æ‰§è¡Œæ•°æ®çˆ¬å–**
   ```python
   # å®Œæ•´çˆ¬å–
   service.run_full_crawl()
   
   # æˆ–å•ç‹¬çˆ¬å–
   service.run_stock_list_crawl()
   ```

4. **æ•°æ®å­˜å‚¨æŸ¥è¯¢**
   ```python
   storage_info = service.get_storage_info()
   print(storage_info)
   ```

## ğŸ› ï¸ æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„å­˜å‚¨ç±»å‹ï¼š

1. åœ¨ `DataRepository` ä¸­æ·»åŠ æ–°çš„å­˜å‚¨ç±»å‹åˆ¤æ–­
2. å®ç°å¯¹åº”çš„å­˜å‚¨ç®¡ç†å™¨ç±»
3. æ›´æ–°é…ç½®æ–‡ä»¶

### æ·»åŠ æ–°çš„çˆ¬è™«æœåŠ¡ï¼š

1. åœ¨ `crawlers/` ç›®å½•åˆ›å»ºæ–°çš„çˆ¬è™«ç±»
2. ç»§æ‰¿ `BaseCrawler` åŸºç±»
3. åœ¨ `CrawlerService` ä¸­æ·»åŠ å¯¹åº”æ–¹æ³•

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **Cookieç®¡ç†**: Cookieä¼šè‡ªåŠ¨è¿‡æœŸï¼Œéœ€è¦å®šæœŸæ›´æ–°
2. **è¿æ¥æ± **: ä½¿ç”¨å®Œæ¯•åå»ºè®®è°ƒç”¨ `close_all_connections()`
3. **å¤§æ–‡ä»¶**: CSVæ–‡ä»¶è¶…è¿‡50MBä¼šè‡ªåŠ¨åˆ†å—å¤„ç†
4. **æ—¥å¿—çº§åˆ«**: ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ `INFO` çº§åˆ«
5. **ä¼šè¯æ¸…ç†**: é•¿æœŸè¿è¡Œçš„åº”ç”¨å»ºè®®å®šæœŸè°ƒç”¨ `cleanup_session()`

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜ï¼š

1. **CookieéªŒè¯å¤±è´¥**
   - æ£€æŸ¥ç½‘ç»œè¿æ¥
   - é‡æ–°è¿è¡Œ `python get_cookie.py`

2. **æ•°æ®åº“è¿æ¥é”™è¯¯**
   - æ£€æŸ¥æ•°æ®åº“é…ç½®
   - ç¡®è®¤æ•°æ®åº“æœåŠ¡çŠ¶æ€

3. **CSVæ–‡ä»¶æƒé™é”™è¯¯**
   - æ£€æŸ¥ç›®å½•æƒé™
   - ç¡®è®¤ç£ç›˜ç©ºé—´å……è¶³

4. **JavaScriptæ‰§è¡Œè¶…æ—¶**
   - æ£€æŸ¥Node.jså®‰è£…
   - ç½‘ç»œè¿æ¥é—®é¢˜

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. æ—¥å¿—æ–‡ä»¶ï¼š`logs/xueqiu_crawler.log`
2. é…ç½®æ–‡ä»¶ï¼š`config/settings.py`
3. CookieçŠ¶æ€ï¼šè¿è¡Œè®¤è¯æµ‹è¯•

---

*æœ€åæ›´æ–°ï¼š2025-11-22*