#!/usr/bin/env python3
"""
é›ªçƒè‚¡ç¥¨æ•°æ®çˆ¬è™«
ä¸»è¦åŠŸèƒ½ï¼š
Step 1: æµ‹è¯•è®¤è¯çŠ¶æ€
    è‹¥è®¤è¯çŠ¶æ€å¼‚å¸¸ï¼Œæç¤ºå…ˆè·å–é›ªçƒCookieï¼Œè¿è¡Œget_cookie.py
    è‹¥è®¤è¯çŠ¶æ€æ­£å¸¸ï¼Œè¿›å…¥Step 2
Step 2: æ ¹æ®é€‰é¡¹çˆ¬å–æŒ‡å®šæ•°æ®ï¼Œå¹¶ä¿å­˜åˆ°æ•°æ®åº“
    é€‰é¡¹å¦‚ä¸‹ï¼š
    1. çˆ¬å–å…¬å¸ä¿¡æ¯
    2. çˆ¬å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
    3. çˆ¬å–Kçº¿æ•°æ®ï¼ˆæŒ‰æ—¥æœŸå­˜å‚¨ï¼‰
    4. çˆ¬å–è´¢åŠ¡æ•°æ®ï¼ˆæŒ‰è¯åˆ¸ä»£ç å­˜å‚¨ï¼‰
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from crawlers.stock_info_crawler import StockInfoCrawler
from crawlers.kline_crawler import KlineCrawler
from crawlers.company_info_crawler import CompanyInfoCrawler
from crawlers.financial_crawler import FinancialCrawler
from crawlers.financial_statements_crawler import FinancialStatementsCrawler
from engine.database import DataRepository
from engine.xueqiu_auth import get_auth

def test_authentication():
    """æµ‹è¯•è®¤è¯çŠ¶æ€"""
    print("\nğŸ” æµ‹è¯•è®¤è¯çŠ¶æ€...")
    print("-" * 30)
    
    auth = get_auth()
    cookies = auth.get_cookies()
    
    if not cookies:
        print("âŒ è®¤è¯çŠ¶æ€å¼‚å¸¸ï¼šæœªæ‰¾åˆ°Cookie")
        print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š")
        print("è¯·å…ˆè·å–é›ªçƒCookieï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š")
        print("   python engine/get_cookie.py")
        print("\nğŸ“– è·å–Cookieå¼•å¯¼ï¼š")
        print("   1. æµè§ˆå™¨ç™»å½•é›ªçƒï¼šhttps://xueqiu.com")
        print("   2. F12 â†’ Network â†’ å¤åˆ¶Cookieå­—ç¬¦ä¸²")
        print("   3. ç²˜è´´åˆ° cookie_input.txt æ–‡ä»¶")
        print("   4. é‡æ–°è¿è¡Œ get_cookie.py")
        return False
    
    # æ£€æŸ¥ç”¨æˆ·ID
    user_id = cookies.get('u', '0')
    if user_id == '0':
        print("âš ï¸  è®¤è¯çŠ¶æ€å¼‚å¸¸ï¼šæ¸¸å®¢æ¨¡å¼ï¼ˆu=0ï¼‰")
        print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š")
        print("è¯·ä½¿ç”¨ç™»å½•åçš„Cookieï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤é‡æ–°è·å–ï¼š")
        print("   python engine/get_cookie.py")
        return False
    
    print(f"âœ… è®¤è¯çŠ¶æ€æ­£å¸¸ï¼šç”¨æˆ·ID {user_id}")
    
    # éªŒè¯Cookieæœ‰æ•ˆæ€§
    if auth._validate_cookies(cookies):
        print("âœ… CookieéªŒè¯é€šè¿‡")
        return True
    else:
        print("âš ï¸  CookieéªŒè¯å¤±è´¥")
        print("ğŸ’¡ å»ºè®®é‡æ–°è·å–Cookieï¼špython engine/get_cookie.py")
        return False


def show_menu():
    """æ˜¾ç¤ºä¸»èœå•"""
    print("\n" + "="*50)
    print("ğŸš€ é›ªçƒè‚¡ç¥¨æ•°æ®çˆ¬è™«")
    print("="*50)
    print("1. çˆ¬å–å…¬å¸ä¿¡æ¯ï¼ˆä¸å¿…éœ€ï¼‰")
    print("2. è·å–è‚¡ç¥¨ä¿¡æ¯ï¼ˆå®Œæ•´å­—æ®µï¼‰")
    print("3. åˆ›å»ºè‚¡ç¥¨åˆ—è¡¨ï¼ˆç®€åŒ–å­—æ®µï¼Œä¸å¿…éœ€ï¼‰")
    print("4. çˆ¬å–æ—¥é¢‘Kçº¿æ•°æ®ï¼ˆæŒ‰æ—¥æœŸå­˜å‚¨ï¼‰")
    print("5. çˆ¬å–è´¢åŠ¡æ•°æ®ï¼ˆæŒ‰è¯åˆ¸ä»£ç å­˜å‚¨ï¼‰")
    print("6. çˆ¬å–è´¢åŠ¡æŠ¥è¡¨ï¼ˆä¸‰è¡¨å®Œæ•´æ•°æ®ï¼‰")
    print("7. çˆ¬å–æ‰€æœ‰æ•°æ®")
    print("0. é€€å‡º")
    print("="*50)

def crawl_company_info():
    """çˆ¬å–å…¬å¸ä¿¡æ¯"""
    print("\nğŸ¢ å…¬å¸ä¿¡æ¯çˆ¬å–é€‰é¡¹")
    print("=" * 40)
    print("1. çˆ¬å–æ‰€æœ‰å…¬å¸ä¿¡æ¯")
    print("2. æŒ‰è¯åˆ¸ä»£ç çˆ¬å–å•ä¸ªå…¬å¸ä¿¡æ¯")
    print("3. æ‰¹é‡çˆ¬å–æŒ‡å®šå…¬å¸ä¿¡æ¯")
    print("4. æŸ¥çœ‹æŒ‡å®šå…¬å¸ä¿¡æ¯")
    print("5. æ›´æ–°æŒ‡å®šå…¬å¸ä¿¡æ¯")
    print("6. å¯¼å‡ºå…¬å¸ä¿¡æ¯åˆ°CSV")
    print("0. è¿”å›ä¸»èœå•")
    
    choice = input("\nè¯·é€‰æ‹© (0-6): ").strip()
    
    if choice == '0':
        return
    elif choice == '1':
        print("\nğŸ¢ å¼€å§‹çˆ¬å–æ‰€æœ‰å…¬å¸ä¿¡æ¯...")
        crawler = CompanyInfoCrawler(DataRepository())
        result = crawler.crawl_company_info()
        print(f"âœ… å…¬å¸ä¿¡æ¯çˆ¬å–å®Œæˆï¼æˆåŠŸ: {result['success']}, å¤±è´¥: {result['error']}")
    elif choice == '2':
        symbol = input("è¯·è¾“å…¥è¯åˆ¸ä»£ç  (å¦‚ SZ000001): ").strip()
        if symbol:
            print(f"\nğŸ¢ å¼€å§‹çˆ¬å–å…¬å¸ä¿¡æ¯: {symbol}")
            crawler = CompanyInfoCrawler(DataRepository())
            result = crawler.crawl_company_info_by_code(symbol)
            if result:
                print(f"âœ… å…¬å¸ä¿¡æ¯çˆ¬å–æˆåŠŸ: {symbol} - {result.get('compsname', '')}")
            else:
                print(f"âŒ å…¬å¸ä¿¡æ¯çˆ¬å–å¤±è´¥: {symbol}")
        else:
            print("âŒ è¯åˆ¸ä»£ç ä¸èƒ½ä¸ºç©º")
    elif choice == '3':
        symbols_input = input("è¯·è¾“å…¥è¯åˆ¸ä»£ç åˆ—è¡¨ (ç”¨é€—å·åˆ†éš”ï¼Œå¦‚ SZ000001,SH600001): ").strip()
        if symbols_input:
            symbols = [s.strip() for s in symbols_input.split(',') if s.strip()]
            print(f"\nğŸ¢ å¼€å§‹æ‰¹é‡çˆ¬å–å…¬å¸ä¿¡æ¯ï¼Œå…±{len(symbols)}æ”¯è‚¡ç¥¨...")
            crawler = CompanyInfoCrawler(DataRepository())
            result = crawler.crawl_company_info_batch(symbols)
            print(f"âœ… æ‰¹é‡çˆ¬å–å®Œæˆï¼æˆåŠŸ: {result['success']}, å¤±è´¥: {result['error']}")
        else:
            print("âŒ è¯åˆ¸ä»£ç åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
    elif choice == '4':
        symbol = input("è¯·è¾“å…¥è¯åˆ¸ä»£ç  (å¦‚ SZ000001): ").strip()
        if symbol:
            print(f"\nğŸ” æŸ¥è¯¢å…¬å¸ä¿¡æ¯: {symbol}")
            crawler = CompanyInfoCrawler(DataRepository())
            info = crawler.get_company_info_by_symbol(symbol)
            if info:
                print(f"âœ… æ‰¾åˆ°å…¬å¸ä¿¡æ¯:")
                print(f"   è¯åˆ¸ä»£ç : {info.get('compcode', '')}")
                print(f"   å…¬å¸åç§°: {info.get('compsname', '')}")
                print(f"   æ³•å®šåç§°: {info.get('compname', '')}")
                print(f"   è‹±æ–‡åç§°: {info.get('engname', '')}")
                print(f"   æˆç«‹æ—¶é—´: {info.get('founddate', '')}")
                print(f"   æ³¨å†Œèµ„æœ¬: {info.get('regcapital', '')}")
                print(f"   è‘£äº‹é•¿: {info.get('chairman', '')}")
                print(f"   æ€»ç»ç†: {info.get('manager', '')}")
                print(f"   æ³¨å†Œåœ°å€: {info.get('regaddr', '')}")
                print(f"   åŠå…¬åœ°å€: {info.get('officeaddr', '')}")
                print(f"   æ›´æ–°æ—¶é—´: {info.get('updated_at', '')}")
            else:
                print(f"âŒ æœªæ‰¾åˆ°å…¬å¸ä¿¡æ¯: {symbol}")
        else:
            print("âŒ è¯åˆ¸ä»£ç ä¸èƒ½ä¸ºç©º")
    elif choice == '5':
        symbol = input("è¯·è¾“å…¥è¯åˆ¸ä»£ç  (å¦‚ SZ000001): ").strip()
        if symbol:
            print(f"\nğŸ”„ æ›´æ–°å…¬å¸ä¿¡æ¯: {symbol}")
            crawler = CompanyInfoCrawler(DataRepository())
            result = crawler.update_company_info_by_symbol(symbol)
            if result:
                print(f"âœ… å…¬å¸ä¿¡æ¯æ›´æ–°æˆåŠŸ: {symbol} - {result.get('compsname', '')}")
            else:
                print(f"âŒ å…¬å¸ä¿¡æ¯æ›´æ–°å¤±è´¥: {symbol}")
        else:
            print("âŒ è¯åˆ¸ä»£ç ä¸èƒ½ä¸ºç©º")
    elif choice == '6':
        print("\nğŸ“„ å¯¼å‡ºå…¬å¸ä¿¡æ¯é€‰é¡¹")
        print("1. å¯¼å‡ºæ‰€æœ‰å…¬å¸ä¿¡æ¯")
        print("2. å¯¼å‡ºæŒ‡å®šå…¬å¸ä¿¡æ¯")
        export_choice = input("è¯·é€‰æ‹© (1-2): ").strip()
        
        if export_choice == '1':
            print("\nğŸ“„ å¯¼å‡ºæ‰€æœ‰å…¬å¸ä¿¡æ¯...")
            crawler = CompanyInfoCrawler(DataRepository())
            success = crawler.export_company_info_to_csv()
            if success:
                print("âœ… æ‰€æœ‰å…¬å¸ä¿¡æ¯å¯¼å‡ºæˆåŠŸï¼")
            else:
                print("âŒ å…¬å¸ä¿¡æ¯å¯¼å‡ºå¤±è´¥ï¼")
        elif export_choice == '2':
            symbols_input = input("è¯·è¾“å…¥è¯åˆ¸ä»£ç åˆ—è¡¨ (ç”¨é€—å·åˆ†éš”): ").strip()
            if symbols_input:
                symbols = [s.strip() for s in symbols_input.split(',') if s.strip()]
                print(f"\nğŸ“„ å¯¼å‡ºæŒ‡å®šå…¬å¸ä¿¡æ¯ï¼Œå…±{len(symbols)}æ”¯è‚¡ç¥¨...")
                crawler = CompanyInfoCrawler(DataRepository())
                success = crawler.export_company_info_to_csv(symbols=symbols)
                if success:
                    print("âœ… æŒ‡å®šå…¬å¸ä¿¡æ¯å¯¼å‡ºæˆåŠŸï¼")
                else:
                    print("âŒ å…¬å¸ä¿¡æ¯å¯¼å‡ºå¤±è´¥ï¼")
            else:
                print("âŒ è¯åˆ¸ä»£ç åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")

def get_stock_info():
    """è·å–è‚¡ç¥¨ä¿¡æ¯ï¼ˆå®Œæ•´å­—æ®µï¼Œä¿å­˜åˆ°stock_infoï¼‰"""
    print("\nğŸ“ˆ è·å–è‚¡ç¥¨ä¿¡æ¯é€‰é¡¹")
    print("=" * 40)
    print("1. è·å–ä»Šæ—¥è‚¡ç¥¨ä¿¡æ¯")
    print("2. è·å–æŒ‡å®šæ—¥æœŸè‚¡ç¥¨ä¿¡æ¯")
    print("3. æŸ¥çœ‹æŒ‡å®šæ—¥æœŸè‚¡ç¥¨ä¿¡æ¯")
    print("0. è¿”å›ä¸»èœå•")
    
    choice = input("\nè¯·é€‰æ‹© (0-3): ").strip()
    
    if choice == '0':
        return
    elif choice == '1':
        print("\nğŸ“ˆ å¼€å§‹è·å–ä»Šæ—¥è‚¡ç¥¨ä¿¡æ¯...")
        crawler = StockInfoCrawler(DataRepository())
        crawler.crawl_stock_list()
        print("âœ… ä»Šæ—¥è‚¡ç¥¨ä¿¡æ¯è·å–å®Œæˆï¼")
    elif choice == '2':
        date_str = input("è¯·è¾“å…¥æ—¥æœŸ (YYYY-MM-DDï¼Œå¦‚ 2024-01-01): ").strip()
        if date_str:
            print(f"\nğŸ“ˆ å¼€å§‹è·å– {date_str} çš„è‚¡ç¥¨ä¿¡æ¯...")
            # æ³¨æ„ï¼šå½“å‰stock_info_crawleråªæ”¯æŒè·å–å½“å¤©æ•°æ®
            # è¿™é‡Œå¯ä»¥æç¤ºç”¨æˆ·æˆ–ä¿®æ”¹çˆ¬è™«ä»¥æ”¯æŒæŒ‡å®šæ—¥æœŸ
            print("âš ï¸  æ³¨æ„ï¼šå½“å‰ç‰ˆæœ¬åªæ”¯æŒè·å–å½“å¤©æ•°æ®")
            crawler = StockInfoCrawler(DataRepository())
            crawler.crawl_stock_list()
            print("âœ… è‚¡ç¥¨ä¿¡æ¯è·å–å®Œæˆï¼")
        else:
            print("âŒ æ—¥æœŸä¸èƒ½ä¸ºç©º")
    elif choice == '3':
        date_str = input("è¯·è¾“å…¥æ—¥æœŸ (YYYY-MM-DDï¼Œç•™ç©ºä¸ºä»Šå¤©): ").strip()
        if not date_str:
            date_str = None
        print(f"\nğŸ” æŸ¥çœ‹è‚¡ç¥¨ä¿¡æ¯ï¼Œæ—¥æœŸ: {date_str or 'ä»Šå¤©'}")
        crawler = StockInfoCrawler(DataRepository())
        if hasattr(crawler.data_repo, 'csv_storage') and crawler.data_repo.csv_storage:
            stocks = crawler.data_repo.csv_storage.get_stock_info_by_date(date_str or '2025-11-22')
            if stocks:
                print(f"âœ… æ‰¾åˆ° {len(stocks)} æ¡è‚¡ç¥¨è®°å½•")
                print("\nå‰10æ¡è®°å½•:")
                print("-" * 80)
                for i, stock in enumerate(stocks[:10], 1):
                    print(f"{i:2d}. {stock.get('symbol', ''):<10} {stock.get('name', ''):<15} "
                          f"ä»·æ ¼:{stock.get('current', 0):>8.2f} "
                          f"æ¶¨è·Œ:{stock.get('percent', 0):>6.2f}% "
                          f"æˆäº¤é‡:{stock.get('volume', 0):>10,}")
                if len(stocks) > 10:
                    print(f"... è¿˜æœ‰ {len(stocks) - 10} æ¡è®°å½•")
            else:
                print(f"âŒ æœªæ‰¾åˆ° {date_str or 'ä»Šå¤©'} çš„è‚¡ç¥¨ä¿¡æ¯")
        else:
            print("âŒ å½“å‰ä¸æ”¯æŒæ•°æ®åº“æ¨¡å¼æŸ¥çœ‹")
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")

def create_stock_list():
    """åˆ›å»ºè‚¡ç¥¨åˆ—è¡¨ï¼ˆç®€åŒ–å­—æ®µï¼Œä¿å­˜åˆ°stock_listï¼‰"""
    print("\nğŸ“‹ åˆ›å»ºè‚¡ç¥¨åˆ—è¡¨é€‰é¡¹")
    print("=" * 40)
    print("1. ä»ä»Šæ—¥stock_infoåˆ›å»ºç®€åŒ–åˆ—è¡¨")
    print("2. ä»æŒ‡å®šæ—¥æœŸstock_infoåˆ›å»ºç®€åŒ–åˆ—è¡¨")
    print("3. æŸ¥çœ‹æŒ‡å®šæ—¥æœŸè‚¡ç¥¨åˆ—è¡¨")
    print("0. è¿”å›ä¸»èœå•")
    
    choice = input("\nè¯·é€‰æ‹© (0-3): ").strip()
    
    if choice == '0':
        return
    elif choice == '1':
        print("\nğŸ“‹ ä»ä»Šæ—¥stock_infoåˆ›å»ºç®€åŒ–è‚¡ç¥¨åˆ—è¡¨...")
        crawler = StockInfoCrawler(DataRepository())
        result = crawler.create_simplified_stock_list()
        if result:
            print("âœ… ä»Šæ—¥ç®€åŒ–è‚¡ç¥¨åˆ—è¡¨åˆ›å»ºå®Œæˆï¼")
        else:
            print("âŒ ä»Šæ—¥ç®€åŒ–è‚¡ç¥¨åˆ—è¡¨åˆ›å»ºå¤±è´¥ï¼")
    elif choice == '2':
        date_str = input("è¯·è¾“å…¥æ—¥æœŸ (YYYY-MM-DDï¼Œå¦‚ 2024-01-01): ").strip()
        if date_str:
            print(f"\nğŸ“‹ ä» {date_str} çš„stock_infoåˆ›å»ºç®€åŒ–è‚¡ç¥¨åˆ—è¡¨...")
            crawler = StockInfoCrawler(DataRepository())
            result = crawler.create_simplified_stock_list(date_str)
            if result:
                print(f"âœ… {date_str} ç®€åŒ–è‚¡ç¥¨åˆ—è¡¨åˆ›å»ºå®Œæˆï¼")
            else:
                print(f"âŒ {date_str} ç®€åŒ–è‚¡ç¥¨åˆ—è¡¨åˆ›å»ºå¤±è´¥ï¼")
        else:
            print("âŒ æ—¥æœŸä¸èƒ½ä¸ºç©º")
    elif choice == '3':
        date_str = input("è¯·è¾“å…¥æ—¥æœŸ (YYYY-MM-DDï¼Œç•™ç©ºä¸ºä»Šå¤©): ").strip()
        if not date_str:
            date_str = None
        print(f"\nğŸ” æŸ¥çœ‹è‚¡ç¥¨åˆ—è¡¨ï¼Œæ—¥æœŸ: {date_str or 'ä»Šå¤©'}")
        crawler = StockInfoCrawler(DataRepository())
        if hasattr(crawler.data_repo, 'csv_storage') and crawler.data_repo.csv_storage:
            stocks = crawler.data_repo.csv_storage.get_stock_list_by_date(date_str or '2025-11-22')
            if stocks:
                print(f"âœ… æ‰¾åˆ° {len(stocks)} æ¡è‚¡ç¥¨è®°å½•")
                print("\nå‰10æ¡è®°å½•:")
                print("-" * 60)
                for i, stock in enumerate(stocks[:10], 1):
                    print(f"{i:2d}. {stock.get('symbol', ''):<10} {stock.get('name', ''):<15} "
                          f"æ›´æ–°æ—¶é—´: {stock.get('crawl_time', '')}")
                if len(stocks) > 10:
                    print(f"... è¿˜æœ‰ {len(stocks) - 10} æ¡è®°å½•")
            else:
                print(f"âŒ æœªæ‰¾åˆ° {date_str or 'ä»Šå¤©'} çš„è‚¡ç¥¨åˆ—è¡¨")
        else:
            print("âŒ å½“å‰ä¸æ”¯æŒæ•°æ®åº“æ¨¡å¼æŸ¥çœ‹")
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")

def crawl_kline_data():
    """çˆ¬å–Kçº¿æ•°æ®ï¼ˆæŒ‰æ—¥æœŸå­˜å‚¨ï¼‰"""
    print("\nğŸ“Š Kçº¿æ•°æ®çˆ¬å–é€‰é¡¹")
    print("=" * 40)
    print("1. çˆ¬å–æ‰€æœ‰è‚¡ç¥¨Kçº¿æ•°æ®ï¼ˆåå¤æƒï¼‰")
    print("2. çˆ¬å–æ‰€æœ‰è‚¡ç¥¨Kçº¿æ•°æ®ï¼ˆå‰å¤æƒï¼‰")
    print("3. çˆ¬å–æ‰€æœ‰è‚¡ç¥¨Kçº¿æ•°æ®ï¼ˆä¸å¤æƒï¼‰")
    print("4. çˆ¬å–æŒ‡å®šæ•°é‡è‚¡ç¥¨Kçº¿æ•°æ®")
    print("5. çˆ¬å–å•åªè‚¡ç¥¨Kçº¿æ•°æ®")
    print("6. æ¢å¤çˆ¬å–ï¼ˆåªå¤„ç†æœªå®Œæˆçš„è‚¡ç¥¨ï¼‰")
    print("7. ğŸ†• çˆ¬å–å…¨å¸‚åœºè‚¡ç¥¨æŸæ—¥æ•°æ®")
    print("8. æŸ¥çœ‹Kçº¿æ•°æ®")
    print("9. æŸ¥çœ‹å¤„ç†è¿›åº¦")
    print("0. è¿”å›ä¸»èœå•")
    
    choice = input("\nè¯·é€‰æ‹© (0-9): ").strip()
    
    if choice == '0':
        return
    elif choice == '1':
        print("\nğŸ“Š å¼€å§‹çˆ¬å–æ‰€æœ‰è‚¡ç¥¨Kçº¿æ•°æ®ï¼ˆåå¤æƒï¼‰...")
        crawler = KlineCrawler(DataRepository())
        crawler.crawl_kline_data('after')
        print("âœ… Kçº¿æ•°æ®çˆ¬å–å®Œæˆï¼")
    elif choice == '2':
        print("\nğŸ“Š å¼€å§‹çˆ¬å–æ‰€æœ‰è‚¡ç¥¨Kçº¿æ•°æ®ï¼ˆå‰å¤æƒï¼‰...")
        crawler = KlineCrawler(DataRepository())
        crawler.crawl_kline_data('before')
        print("âœ… Kçº¿æ•°æ®çˆ¬å–å®Œæˆï¼")
    elif choice == '3':
        print("\nğŸ“Š å¼€å§‹çˆ¬å–æ‰€æœ‰è‚¡ç¥¨Kçº¿æ•°æ®ï¼ˆä¸å¤æƒï¼‰...")
        crawler = KlineCrawler(DataRepository())
        crawler.crawl_kline_data('none')
        print("âœ… Kçº¿æ•°æ®çˆ¬å–å®Œæˆï¼")
    elif choice == '4':
        try:
            max_stocks = int(input("è¯·è¾“å…¥è¦çˆ¬å–çš„è‚¡ç¥¨æ•°é‡: ").strip())
            if max_stocks > 0:
                print(f"\nğŸ“Š å¼€å§‹çˆ¬å– {max_stocks} åªè‚¡ç¥¨Kçº¿æ•°æ®ï¼ˆåå¤æƒï¼‰...")
                crawler = KlineCrawler(DataRepository())
                crawler.crawl_kline_data('after', max_stocks)
                print("âœ… Kçº¿æ•°æ®çˆ¬å–å®Œæˆï¼")
            else:
                print("âŒ è‚¡ç¥¨æ•°é‡å¿…é¡»å¤§äº0")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
    elif choice == '5':
        symbol = input("è¯·è¾“å…¥è‚¡ç¥¨ä»£ç  (å¦‚ SZ000001): ").strip()
        if symbol:
            print(f"\nğŸ“Š å¼€å§‹çˆ¬å– {symbol} Kçº¿æ•°æ®ï¼ˆåå¤æƒï¼‰...")
            crawler = KlineCrawler(DataRepository())
            kline_data = crawler.crawl_single_stock_kline(symbol, 'after')
            if kline_data:
                print(f"âœ… æˆåŠŸè·å– {len(kline_data)} æ¡Kçº¿æ•°æ®")
                print("æœ€æ–°5æ¡æ•°æ®:")
                for data in kline_data[-5:]:
                    date = data.get('crawl_date', '')
                    close = data.get('close', 0)
                    percent = data.get('percent', 0)
                    print(f"  {date}: æ”¶ç›˜ä»· {close}, æ¶¨è·Œå¹… {percent}%")
            else:
                print(f"âŒ æœªè·å–åˆ° {symbol} çš„Kçº¿æ•°æ®")
        else:
            print("âŒ è‚¡ç¥¨ä»£ç ä¸èƒ½ä¸ºç©º")
    elif choice == '6':
        print("\nğŸ“Š æ¢å¤Kçº¿æ•°æ®çˆ¬å–...")
        crawler = KlineCrawler(DataRepository())
        crawler.resume_crawl('after')
        print("âœ… æ¢å¤çˆ¬å–å®Œæˆï¼")
    elif choice == '7':
        # ğŸ†• çˆ¬å–å…¨å¸‚åœºè‚¡ç¥¨æŸæ—¥æ•°æ®
        date_str = input("è¯·è¾“å…¥ç›®æ ‡æ—¥æœŸ (YYYY-MM-DDï¼Œç•™ç©ºä¸ºä»Šå¤©): ").strip()
        if not date_str:
            from datetime import datetime
            date_str = datetime.now().strftime('%Y-%m-%d')
        
        adjust_choice = input("è¯·é€‰æ‹©å¤æƒç±»å‹ (1-å‰å¤æƒ, 2-åå¤æƒ, 3-ä¸å¤æƒï¼Œé»˜è®¤åå¤æƒ): ").strip()
        adjust_type = 'after'  # é»˜è®¤åå¤æƒ
        if adjust_choice == '1':
            adjust_type = 'before'
        elif adjust_choice == '3':
            adjust_type = 'none'
        
        max_stocks_input = input("é™åˆ¶è‚¡ç¥¨æ•°é‡ (ç•™ç©ºå¤„ç†å…¨éƒ¨): ").strip()
        max_stocks = None
        if max_stocks_input and max_stocks_input.isdigit():
            max_stocks = int(max_stocks_input)
        
        print(f"\nğŸ“Š å¼€å§‹çˆ¬å–å…¨å¸‚åœºè‚¡ç¥¨ {date_str} æ—¥é¢‘æ•°æ®ï¼ˆ{adjust_type}å¤æƒï¼‰...")
        crawler = KlineCrawler(DataRepository())
        crawler.crawl_market_daily_data(date_str, adjust_type, max_stocks)
        print("âœ… å…¨å¸‚åœºæ—¥é¢‘æ•°æ®çˆ¬å–å®Œæˆï¼")
    elif choice == '8':
        date_str = input("è¯·è¾“å…¥æ—¥æœŸ (YYYY-MM-DDï¼Œç•™ç©ºä¸ºä»Šå¤©): ").strip()
        if not date_str:
            from datetime import datetime
            date_str = datetime.now().strftime('%Y-%m-%d')
        
        print(f"\nğŸ” æŸ¥çœ‹ {date_str} çš„Kçº¿æ•°æ®...")
        if hasattr(DataRepository(), 'csv_storage'):
            csv_storage = DataRepository().csv_storage
            kline_data = csv_storage.get_kline_data_by_date(date_str)
            if kline_data:
                print(f"âœ… æ‰¾åˆ° {len(kline_data)} æ¡Kçº¿è®°å½•")
                print("\næœ€æ–°10æ¡è®°å½•:")
                print("-" * 80)
                for i, data in enumerate(kline_data[-10:], 1):
                    symbol = data.get('symbol', '')
                    date = data.get('crawl_date', '')
                    close = data.get('close', 0)
                    volume = data.get('volume', 0)
                    percent = data.get('percent', 0)
                    print(f"{i:2d}. {symbol:<10} {date} æ”¶ç›˜:{float(close):>8.2f} "
                          f"æ¶¨è·Œ:{float(percent):>6.2f}% æˆäº¤é‡:{int(volume):>10,}")
            else:
                print(f"âŒ æœªæ‰¾åˆ° {date_str} çš„Kçº¿æ•°æ®")
        else:
            print("âŒ å½“å‰ä¸æ”¯æŒæ•°æ®åº“æ¨¡å¼æŸ¥çœ‹")
    elif choice == '9':
        print("\nğŸ“Š æŸ¥çœ‹Kçº¿æ•°æ®å¤„ç†è¿›åº¦...")
        crawler = KlineCrawler(DataRepository())
        processed_symbols = crawler.get_processed_symbols()
        all_symbols = crawler._get_unprocessed_stocks()
        
        if all_symbols:
            processed_count = len(processed_symbols)
            total_count = len(all_symbols)
            progress = (processed_count / total_count * 100) if total_count > 0 else 0
            
            print(f"âœ… å¤„ç†è¿›åº¦: {processed_count}/{total_count} ({progress:.1f}%)")
            print(f"ğŸ“Š å·²å¤„ç†è‚¡ç¥¨: {len(processed_symbols)} åª")
            print(f"â³ å¾…å¤„ç†è‚¡ç¥¨: {len(all_symbols) - len(processed_symbols)} åª")
            
            if processed_symbols:
                print(f"\nå·²å¤„ç†çš„è‚¡ç¥¨ä»£ç ï¼ˆå‰10åªï¼‰:")
                for symbol in processed_symbols[:10]:
                    print(f"  âœ… {symbol}")
                if len(processed_symbols) > 10:
                    print(f"  ... è¿˜æœ‰ {len(processed_symbols) - 10} åª")
        else:
            print("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œè¯·å…ˆè·å–è‚¡ç¥¨ä¿¡æ¯")
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")

def crawl_financial_data():
    """çˆ¬å–è´¢åŠ¡æ•°æ®ï¼ˆæŒ‰è¯åˆ¸ä»£ç å­˜å‚¨ï¼‰"""
    print("\nğŸ’° å¼€å§‹çˆ¬å–è´¢åŠ¡æ•°æ®ï¼ˆæŒ‰è¯åˆ¸ä»£ç å­˜å‚¨ï¼‰...")
    crawler = FinancialCrawler(DataRepository())
    crawler.crawl_financial_data()
    print("âœ… è´¢åŠ¡æ•°æ®çˆ¬å–å®Œæˆï¼")

def crawl_financial_statements():
    """çˆ¬å–è´¢åŠ¡æŠ¥è¡¨ï¼ˆä¸‰è¡¨å®Œæ•´æ•°æ®ï¼‰"""
    print("\nğŸ“Š è´¢åŠ¡æŠ¥è¡¨çˆ¬å–é€‰é¡¹")
    print("=" * 40)
    print("1. çˆ¬å–æ‰€æœ‰è‚¡ç¥¨è´¢åŠ¡æŠ¥è¡¨")
    print("2. æŒ‰è¯åˆ¸ä»£ç çˆ¬å–å•ä¸ªè‚¡ç¥¨è´¢åŠ¡æŠ¥è¡¨")
    print("3. æ‰¹é‡çˆ¬å–æŒ‡å®šè‚¡ç¥¨è´¢åŠ¡æŠ¥è¡¨")
    print("0. è¿”å›ä¸»èœå•")
    
    choice = input("\nè¯·é€‰æ‹© (0-3): ").strip()
    
    if choice == '0':
        return
    elif choice == '1':
        print("\nğŸ“Š å¼€å§‹çˆ¬å–æ‰€æœ‰è‚¡ç¥¨è´¢åŠ¡æŠ¥è¡¨...")
        crawler = FinancialStatementsCrawler(DataRepository())
        crawler.crawl_financial_statements()
        print("âœ… è´¢åŠ¡æŠ¥è¡¨çˆ¬å–å®Œæˆï¼")
    elif choice == '2':
        symbol = input("è¯·è¾“å…¥è¯åˆ¸ä»£ç ï¼ˆå¦‚ SH600519ï¼‰: ").strip().upper()
        if symbol:
            print(f"\nğŸ“Š å¼€å§‹çˆ¬å– {symbol} çš„è´¢åŠ¡æŠ¥è¡¨...")
            crawler = FinancialStatementsCrawler(DataRepository())
            success = crawler.crawl_single_stock_statements(symbol)
            if success:
                print(f"âœ… {symbol} è´¢åŠ¡æŠ¥è¡¨çˆ¬å–å®Œæˆï¼")
            else:
                print(f"âŒ {symbol} è´¢åŠ¡æŠ¥è¡¨çˆ¬å–å¤±è´¥ï¼")
        else:
            print("âŒ è¯åˆ¸ä»£ç ä¸èƒ½ä¸ºç©º")
    elif choice == '3':
        symbols_input = input("è¯·è¾“å…¥è¯åˆ¸ä»£ç åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”ï¼ˆå¦‚ SH600519,SZ000001ï¼‰: ").strip().upper()
        if symbols_input:
            symbols = [s.strip() for s in symbols_input.split(',') if s.strip()]
            print(f"\nğŸ“Š å¼€å§‹æ‰¹é‡çˆ¬å–è´¢åŠ¡æŠ¥è¡¨ï¼Œå…± {len(symbols)} åªè‚¡ç¥¨...")
            crawler = FinancialStatementsCrawler(DataRepository())
            success_count = 0
            for i, symbol in enumerate(symbols, 1):
                print(f"\n[{i}/{len(symbols)}] çˆ¬å– {symbol}...")
                try:
                    success = crawler.crawl_single_stock_statements(symbol)
                    if success:
                        success_count += 1
                        print(f"âœ… {symbol} å®Œæˆ")
                    else:
                        print(f"âŒ {symbol} å¤±è´¥")
                except Exception as e:
                    print(f"âŒ {symbol} å¼‚å¸¸: {e}")
            print(f"\nğŸ“Š æ‰¹é‡çˆ¬å–å®Œæˆï¼ŒæˆåŠŸ: {success_count}/{len(symbols)}")
        else:
            print("âŒ è¯åˆ¸ä»£ç åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")



def crawl_all_data():
    """çˆ¬å–æ‰€æœ‰æ•°æ®"""
    print("\nğŸ”„ å¼€å§‹çˆ¬å–æ‰€æœ‰æ•°æ®...")
    
    # 1. å…¬å¸ä¿¡æ¯
    print("1/6 çˆ¬å–å…¬å¸ä¿¡æ¯...")
    company_crawler = CompanyInfoCrawler(DataRepository())
    company_crawler.crawl_company_info()
    
    # 2. è·å–è‚¡ç¥¨ä¿¡æ¯ï¼ˆå®Œæ•´å­—æ®µï¼‰
    print("2/6 è·å–è‚¡ç¥¨ä¿¡æ¯ï¼ˆå®Œæ•´å­—æ®µï¼‰...")
    stock_crawler = StockInfoCrawler(DataRepository())
    stock_crawler.crawl_stock_list()
    
    # 3. åˆ›å»ºè‚¡ç¥¨åˆ—è¡¨ï¼ˆç®€åŒ–å­—æ®µï¼‰
    print("3/6 åˆ›å»ºè‚¡ç¥¨åˆ—è¡¨ï¼ˆç®€åŒ–å­—æ®µï¼‰...")
    stock_crawler.create_simplified_stock_list()
    
    # 4. Kçº¿æ•°æ®
    print("4/6 çˆ¬å–Kçº¿æ•°æ®...")
    kline_crawler = KlineCrawler(DataRepository())
    kline_crawler.crawl_kline_data('after')
    
    # 5. è´¢åŠ¡æ•°æ®
    print("5/6 çˆ¬å–è´¢åŠ¡æ•°æ®...")
    financial_crawler = FinancialCrawler(DataRepository())
    financial_crawler.crawl_financial_data()
    
    # 6. è´¢åŠ¡æŠ¥è¡¨
    print("6/6 çˆ¬å–è´¢åŠ¡æŠ¥è¡¨...")
    statements_crawler = FinancialStatementsCrawler(DataRepository())
    statements_crawler.crawl_financial_statements()
    
    print("âœ… æ‰€æœ‰æ•°æ®çˆ¬å–å®Œæˆï¼")

def main():
    """ä¸»å‡½æ•°"""
    # Step 1: æµ‹è¯•è®¤è¯çŠ¶æ€
    print("ğŸ” Step 1: æµ‹è¯•è®¤è¯çŠ¶æ€")
    if not test_authentication():
        print("\nâŒ è®¤è¯çŠ¶æ€å¼‚å¸¸ï¼Œè¯·å…ˆè§£å†³è®¤è¯é—®é¢˜")
        return
    
    # Step 2: æ˜¾ç¤ºèœå•å¹¶æ‰§è¡Œé€‰æ‹©çš„åŠŸèƒ½
    print("\nğŸ“‹ Step 2: é€‰æ‹©è¦çˆ¬å–çš„æ•°æ®ç±»å‹")
    
    while True:
        show_menu()
        choice = input("è¯·é€‰æ‹©åŠŸèƒ½ (0-7): ").strip()
        
        if choice == '0':
            print("ğŸ‘‹ å†è§ï¼")
            break
        elif choice == '1':
            crawl_company_info()
        elif choice == '2':
            get_stock_info()
        elif choice == '3':
            create_stock_list()
        elif choice == '4':
            crawl_kline_data()
        elif choice == '5':
            crawl_financial_data()
        elif choice == '6':
            crawl_financial_statements()
        elif choice == '7':
            crawl_all_data()
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥ï¼")

if __name__ == "__main__":
    main()